# ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models

The integration of multi-modal large language models (MLLMs) with robotic systems has significantly en- hanced the ability of robots to interpret and act upon nat- ural language instructions. Despite these advancements, con- ventional MLLMs are typically trained on generic image- text pairs, lacking essential robotics knowledge such as af- fordances and physical knowledge, which hampers their effi- cacy in manipulation tasks. To bridge this gap, we introduce ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric knowledge through a Visual Question- Answering format. This approach not only encompasses tool detection and affordance recognition but also extends to a comprehensive understanding of physical concepts. Our methodology entails amassing a diverse array of images fea- turing interactive objects, thereby capturing a wide spectrum of object detection, affordance, and physical concept predic- tion challenges. To seamlessly integrate this robotic-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a unified VQA format and devise a fine- tuning strategy that preserves the original vision-reasoning abilities while incorporating the new robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA.
